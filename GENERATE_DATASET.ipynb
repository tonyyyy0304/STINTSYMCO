{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3fc5a6",
   "metadata": {},
   "source": [
    "# ELI5 Dataset - Gemini Answer Generation\n",
    "\n",
    "This notebook processes the ELI5 (Explain Like I'm 5) dataset and generates answers using Google's Gemini.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: HuggingFace dataset `rexarski/eli5_category`\n",
    "- **Period**: January 2017 - June 2021\n",
    "- **Content**: Human-written questions and answers from the ELI5 subreddit\n",
    "- **Purpose**: Expand dataset with Gemini-generated answers for comparison and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33007cb",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "1. Finalize how what columns to pick for the final dataset\n",
    "    * Current: drop score & subreddit column ONLY\n",
    "2. Finalize what models to use\n",
    "    * Current: Gemini 2.5 flash\n",
    "3. The current gemini df generation only saves the successful attempts to df_gemini (i.e. sample size = 10, but 2 fail, final df length is 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ca00c",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy datasets\n",
    "!pip install google-generativeai\n",
    "!pip install gdown matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d034def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "# Gemini API\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f300bd0",
   "metadata": {},
   "source": [
    "## 2. Set Up Gemini API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"Gemini API key configured successfully\")\n",
    "else:\n",
    "    print(\"Warning: Gemini API key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available Gemini models\n",
    "print(\"Available Gemini models:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"Model: {model.name}\")\n",
    "        print(f\"  Display Name: {model.display_name}\")\n",
    "        print(f\"  Description: {model.description}\")\n",
    "        print(f\"  Supported Methods: {model.supported_generation_methods}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe45f6",
   "metadata": {},
   "source": [
    "## 3. Load the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ab298",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "Download the human answers in the link below and place it in the same folder, then rename to `human_sample.csv`\n",
    "\n",
    "https://drive.google.com/file/u/1/d/1uJG1qKdrO3zJ2npO3xrKfqnb7amqBNfX/view\n",
    "\n",
    "(Can't push the csv file (exceeds 100MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./human_sample.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "print(f\"Dataset loaded with {len(df)} records\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba0500",
   "metadata": {},
   "source": [
    "## 4. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST FEW ROWS\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dc240",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(df_clean)} rows\")\n",
    "\n",
    "# 1. Remove duplicates\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"After removing duplicates: {len(df_clean)} rows\")\n",
    "\n",
    "# 2. Filter by text length (keep only answers with <= 1000 characters)\n",
    "# Justification: Mean is approx. 600 and Median is approx. 400. \n",
    "# Highest is around 9000 and token limit is 1000\n",
    "df_clean = df_clean[df_clean['text'].str.len() <= 1000]\n",
    "print(f\"After filtering text length: {len(df_clean)} rows\")\n",
    "\n",
    "# 3. Drop unnecessary columns (score && subreddit)\n",
    "df_clean = df_clean.drop(columns=['score', 'subreddit'], errors='ignore')\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset size: {len(df_clean)} rows\")\n",
    "print(f\"Removed: {len(df) - len(df_clean)} rows ({((len(df) - len(df_clean)) / len(df) * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5bbce",
   "metadata": {},
   "source": [
    "## 6. Load questions to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64369640",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_clean['title'].tolist()\n",
    "\n",
    "print(\"Question num: \"  + str(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35d9d1",
   "metadata": {},
   "source": [
    "## 7. Generate Answers with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb270de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_answer(question, model=\"gemini-2.5-flash\", max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate an ELI5-style answer using Gemini.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        model: Gemini model to use (default: gemini-2.5-flash)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer as string, or None if failed\n",
    "    \"\"\"\n",
    "    if not GEMINI_API_KEY:\n",
    "        return \"ERROR: Gemini API key not configured\"\n",
    "    \n",
    "    prompt = f\"\"\"You are answering questions in the style of the ELI5 (Explain Like I'm 5) subreddit. \n",
    "Provide a clear, simple explanation that a 5-year-old could understand, but still be informative.\n",
    "Keep everything as one block of text.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model_instance = genai.GenerativeModel(model)\n",
    "            response = model_instance.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=0.7,\n",
    "                    max_output_tokens=1000,\n",
    "                )\n",
    "            )\n",
    "            return response.text.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test the function\n",
    "print(\"TEST\")\n",
    "test_question = \"Why is the sky blue?\"\n",
    "test_answer = generate_gemini_answer(test_question)\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Answer: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gemini answers for the dataset\n",
    "\n",
    "def batch_generate_gemini(df, questions, sample_size=None, delay=1):\n",
    "    # Determine which indices to process\n",
    "    if sample_size is None:\n",
    "        target_count = len(questions)\n",
    "    else:\n",
    "        target_count = sample_size\n",
    "    \n",
    "    successful_rows = []\n",
    "    successful_indices = []  # Track which rows succeeded\n",
    "    idx = 0\n",
    "    \n",
    "    # Keep generating until we have enough successful answers\n",
    "    with tqdm(total=target_count, desc=\"Generating Gemini answers\") as pbar:\n",
    "        while len(successful_rows) < target_count and idx < len(questions):\n",
    "            question = questions[idx]\n",
    "            answer = generate_gemini_answer(question)\n",
    "            \n",
    "            # Only include if answer doesn't start with ERROR\n",
    "            if not answer.startswith(\"ERROR\"):\n",
    "                row = df.iloc[idx].copy()\n",
    "                row['text'] = answer\n",
    "                successful_rows.append(row)\n",
    "                successful_indices.append(idx)\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                print(f\"\\nSkipping row {idx} due to error\")\n",
    "            \n",
    "            idx += 1\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    df_gemini = pd.DataFrame(successful_rows).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nSuccessfully generated {len(df_gemini)} answers\")\n",
    "    if idx >= len(questions) and len(df_gemini) < target_count:\n",
    "        print(f\"Warning: Only got {len(df_gemini)} successful answers out of {target_count} requested\")\n",
    "    \n",
    "    return df_gemini, successful_indices\n",
    "\n",
    "# CHANGE SMAP\n",
    "df_gemini, gemini_indices = batch_generate_gemini(df_clean, questions, sample_size, delay=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b184313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemini.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14c3fc",
   "metadata": {},
   "source": [
    "## 7. Compare Human vs Gemini Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate answer lengths for both human and Gemini datasets\n",
    "def calculate_answer_stats(df_human, df_gemini):\n",
    "    \"\"\"Calculate statistics for human and Gemini-generated answers.\"\"\"\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # Human stats (from df_clean)\n",
    "    if 'text' in df_human.columns:\n",
    "        human_lengths = df_human['text'].astype(str).str.len()\n",
    "        stats['human'] = {\n",
    "            'mean_length': human_lengths.mean(),\n",
    "            'median_length': human_lengths.median(),\n",
    "            'max_length': human_lengths.max(),\n",
    "            'min_length': human_lengths.min()\n",
    "        }\n",
    "    \n",
    "    # Gemini stats (from df_gemini)\n",
    "    if 'text' in df_gemini.columns:\n",
    "        gemini_lengths = df_gemini['text'].astype(str).str.len()\n",
    "        stats['gemini'] = {\n",
    "            'mean_length': gemini_lengths.mean(),\n",
    "            'median_length': gemini_lengths.median(),\n",
    "            'max_length': gemini_lengths.max(),\n",
    "            'min_length': gemini_lengths.min()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate stats\n",
    "stats = calculate_answer_stats(df_clean, df_gemini)\n",
    "\n",
    "# Display stats\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER LENGTH STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "for source, metrics in stats.items():\n",
    "    print(f\"\\n{source.upper()}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize answer length comparison\n",
    "\n",
    "def plot_answer_length_comparison(df_human, df_gemini):\n",
    "    \"\"\"Create visualization comparing answer lengths across different sources.\"\"\"\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    \n",
    "    if 'text' in df_human.columns:\n",
    "        plot_data.append({\n",
    "            'source': 'Human',\n",
    "            'lengths': df_human['text'].astype(str).str.len().tolist()\n",
    "        })\n",
    "    \n",
    "    if 'text' in df_gemini.columns:\n",
    "        plot_data.append({\n",
    "            'source': 'Gemini',\n",
    "            'lengths': df_gemini['text'].astype(str).str.len().tolist()\n",
    "        })\n",
    "    \n",
    "    if not plot_data:\n",
    "        print(\"No data available for plotting yet.\")\n",
    "        return\n",
    "    \n",
    "    # Create box plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    sources = [d['source'] for d in plot_data]\n",
    "    lengths = [d['lengths'] for d in plot_data]\n",
    "    \n",
    "    axes[0].boxplot(lengths, labels=sources)\n",
    "    axes[0].set_title('Answer Length Distribution by Source')\n",
    "    axes[0].set_ylabel('Answer Length (characters)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bar chart of mean lengths\n",
    "    mean_lengths = [np.mean(d['lengths']) for d in plot_data]\n",
    "    axes[1].bar(sources, mean_lengths, color=['blue', 'green'][:len(sources)])\n",
    "    axes[1].set_title('Mean Answer Length by Source')\n",
    "    axes[1].set_ylabel('Mean Length (characters)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "plot_answer_length_comparison(df_clean, df_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2158edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample comparison - View actual answers side by side\n",
    "def compare_answers(df_human, df_gemini, index=0, width=100):\n",
    "    \"\"\"Display a side-by-side comparison of answers for a specific question.\"\"\"\n",
    "    \n",
    "    row_human = df_human.iloc[index]\n",
    "    row_gemini = df_gemini.iloc[index]\n",
    "    \n",
    "    print(f\"QUESTION: {row_human['title']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"-\" * width)\n",
    "    print(\"HUMAN ANSWER\")\n",
    "    print(\"-\" * width)\n",
    "    wrapped_text = textwrap.fill(str(row_human['text']), width=width)\n",
    "    print(wrapped_text)\n",
    "    print()\n",
    "    \n",
    "    print(\"-\" * width)\n",
    "    print(\"GEMINI ANSWER\")\n",
    "    print(\"-\" * width)\n",
    "    wrapped_answer = textwrap.fill(str(row_gemini['text']), width=width)\n",
    "    print(wrapped_answer)\n",
    "    print()\n",
    "\n",
    "# Compare first answer\n",
    "compare_answers(df_clean, df_gemini, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f4a18",
   "metadata": {},
   "source": [
    "## 8. Save Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11842ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enhanced dataset with ALL human and Gemini answers combined and shuffled\n",
    "\n",
    "# Prepare human answers dataset\n",
    "df_human_labeled = df_clean.copy()\n",
    "df_human_labeled['answer_source'] = 'human'\n",
    "\n",
    "# Prepare Gemini answers dataset\n",
    "df_gemini_labeled = df_gemini.copy()\n",
    "df_gemini_labeled['answer_source'] = 'gemini'\n",
    "\n",
    "# Combine ALL answers (human + Gemini)\n",
    "df_combined = pd.concat([df_human_labeled, df_gemini_labeled], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset created:\")\n",
    "print(f\"  Total rows: {len(df_combined)}\")\n",
    "print(f\"  Human answers: {(df_combined['answer_source'] == 'human').sum()} ({(df_combined['answer_source'] == 'human').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"  Gemini answers: {(df_combined['answer_source'] == 'gemini').sum()} ({(df_combined['answer_source'] == 'gemini').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"  Columns: {list(df_combined.columns)}\")\n",
    "print()\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"eli5_combined_{timestamp}.csv\"\n",
    "\n",
    "# Save to CSV\n",
    "df_combined.to_csv(output_filename, index=False)\n",
    "print(f\"Combined dataset saved to: {output_filename}\")\n",
    "\n",
    "# Optional: Save as parquet for better compression and faster loading\n",
    "parquet_filename = f\"eli5_combined_{timestamp}.parquet\"\n",
    "df_combined.to_parquet(parquet_filename, index=False)\n",
    "print(f\"Combined dataset saved to: {parquet_filename}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_filename = f\"eli5_summary_{timestamp}.json\"\n",
    "summary = {\n",
    "    'total_rows': len(df_combined),\n",
    "    'columns': list(df_combined.columns),\n",
    "    'human_answers': int((df_combined['answer_source'] == 'human').sum()),\n",
    "    'gemini_answers': int((df_combined['answer_source'] == 'gemini').sum()),\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary statistics saved to: {summary_filename}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET COMBINATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
