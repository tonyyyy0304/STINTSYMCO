{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d000c52",
   "metadata": {},
   "source": [
    "# ELI5 Dataset - Gemini Answer Generation\n",
    "\n",
    "This notebook processes the ELI5 (Explain Like I'm 5) dataset and generates LLM answers using Google's Gemini.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: HuggingFace dataset `rexarski/eli5_category`\n",
    "- **Period**: January 2017 - June 2021\n",
    "- **Content**: Human-written questions and answers from the ELI5 subreddit\n",
    "- **Purpose**: Generate one LLM answer per unique question and merge with human responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8dbfc",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "\n",
    "**Workflow & Integration:**\n",
    "1. This notebook: Load dataset → Test API → Generate Gemini answers → Merge with human answers\n",
    "2. Model: gemini-2.5-flash\n",
    "3. Each unique question gets ONE LLM answer replicated across all human responses\n",
    "4. **To combine with OpenAI:**\n",
    "   - Run `openai_generate_dataset_clean.ipynb` first (generates human + chatgpt answers)\n",
    "   - Run this notebook (generates human + gemini answers)\n",
    "   - Use the merge function in Cell 10 to combine both without duplicating human answers\n",
    "   - Final dataset: human + chatgpt + gemini sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eec58d",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5518824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas numpy datasets\n",
    "# !pip install google-generativeai\n",
    "# !pip install gdown matplotlib seaborn tqdm\n",
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00d88e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Gemini API\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3dcf0",
   "metadata": {},
   "source": [
    "## 2. Set Up Gemini API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8f0877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API key configured successfully\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv('.env')\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"Gemini API key configured successfully\")\n",
    "else:\n",
    "    print(\"Gemini API key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf2b3b",
   "metadata": {},
   "source": [
    "## 3. Load the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7165f",
   "metadata": {},
   "source": [
    "## Configuration - Set Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6683be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Questions to generate: 4000\n",
      "  Delay between calls: 0.7s\n",
      "  Max concurrent workers: 6\n",
      "  Test sample size: 2\n",
      "  Model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "num_questions_to_generate = 4000\n",
    "delay_between_api_calls = 0.7\n",
    "test_sample_size = 2\n",
    "max_workers = 6  # Number of concurrent API calls\n",
    "\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Questions to generate: {num_questions_to_generate}\")\n",
    "print(f\"  Delay between calls: {delay_between_api_calls}s\")\n",
    "print(f\"  Max concurrent workers: {max_workers}\")\n",
    "print(f\"  Test sample size: {test_sample_size}\")\n",
    "print(f\"  Model: {gemini_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c235c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 228637 records\n",
      "Found 93893 unique questions\n",
      "Average answers per question: 2.4\n"
     ]
    }
   ],
   "source": [
    "path = \"./human_data/output/eli5_cleaned.csv\"\n",
    "\n",
    "df_human = pd.read_csv(path)\n",
    "print(f\"Dataset loaded with {len(df_human)} records\")\n",
    "\n",
    "unique_questions = df_human[['q_id', 'title']].drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Found {len(unique_questions)} unique questions\")\n",
    "print(f\"Average answers per question: {len(df_human) / len(unique_questions):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cc78056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of dataset:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5lchat</td>\n",
       "      <td>Why there was a 'leap second' added to the end of 2016?</td>\n",
       "      <td>the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5lchat</td>\n",
       "      <td>Why there was a 'leap second' added to the end of 2016?</td>\n",
       "      <td>The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5lchat</td>\n",
       "      <td>Why there was a 'leap second' added to the end of 2016?</td>\n",
       "      <td>Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.] In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_id                                                    title  \\\n",
       "0  5lchat  Why there was a 'leap second' added to the end of 2016?   \n",
       "1  5lchat  Why there was a 'leap second' added to the end of 2016?   \n",
       "2  5lchat  Why there was a 'leap second' added to the end of 2016?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.  \n",
       "1  The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.] In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of dataset:\")\n",
    "print(\"=\" * 80)\n",
    "df_human[['q_id', 'title', 'text']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4830ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 existing output file(s):\n",
      "  - eli5_gemini_answers_20260215_230432.csv: 1000 questions\n",
      "\n",
      "Total unique questions already generated: 1000\n",
      "Total unique questions: 93893\n",
      "Already generated: 1000\n",
      "Remaining to generate: 92893\n"
     ]
    }
   ],
   "source": [
    "# Load existing generated answers from output folder to avoid regenerating\n",
    "output_folder = \"gemini-output\"\n",
    "existing_q_ids = set()\n",
    "\n",
    "if os.path.exists(output_folder):\n",
    "    # Find all CSV files in the output folder\n",
    "    csv_files = [f for f in os.listdir(output_folder) if f.startswith('eli5_gemini_answers_') and f.endswith('.csv')]\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"Found {len(csv_files)} existing output file(s):\")\n",
    "        for csv_file in csv_files:\n",
    "            filepath = os.path.join(output_folder, csv_file)\n",
    "            try:\n",
    "                df_existing = pd.read_csv(filepath)\n",
    "                existing_q_ids.update(df_existing['q_id'].unique())\n",
    "                print(f\"  - {csv_file}: {df_existing['q_id'].nunique()} questions\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - {csv_file}: Error reading file - {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nTotal unique questions already generated: {len(existing_q_ids)}\")\n",
    "    else:\n",
    "        print(\"No existing output files found\")\n",
    "else:\n",
    "    print(f\"Output folder '{output_folder}' does not exist - starting fresh\")\n",
    "\n",
    "# Filter out already-processed questions\n",
    "unique_questions_filtered = unique_questions[~unique_questions['q_id'].isin(existing_q_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total unique questions: {len(unique_questions)}\")\n",
    "print(f\"Already generated: {len(existing_q_ids)}\")\n",
    "print(f\"Remaining to generate: {len(unique_questions_filtered)}\")\n",
    "\n",
    "if len(unique_questions_filtered) == 0:\n",
    "    print(\"\\nAll questions have already been processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac5c5a",
   "metadata": {},
   "source": [
    "## 4. Define LLM Answer Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0525088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_answer(question, model=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate an ELI5-style answer using Gemini.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        model: Gemini model to use (default: gemini-2.5-flash)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer as string, or error message if failed\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = gemini_model\n",
    "        \n",
    "    if not GEMINI_API_KEY:\n",
    "        return \"ERROR: Gemini API key not configured\"\n",
    "    \n",
    "    prompt = f\"\"\"You are answering questions in the style of the ELI5 (Explain Like I'm 5) subreddit. \n",
    "Provide a clear, simple explanation that a 5-year-old could understand, but still be informative.\n",
    "Keep everything as one block of text.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model_instance = genai.GenerativeModel(model)\n",
    "            response = model_instance.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=0.7,\n",
    "                    max_output_tokens=1000,\n",
    "                )\n",
    "            )\n",
    "            return response.text.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57ad7f",
   "metadata": {},
   "source": [
    "## 5. Test API with Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a1ca9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API with 2 sample questions...\n",
      "================================================================================\n",
      "\n",
      "[Test 1] Why there was a 'leap second' added to the end of 2016?...\n",
      "Generated (844 chars): Imagine you're running a race, and your friend is also running, but your friend is just a tiny, tiny...\n",
      "\n",
      "[Test 2] How do you claim undiscovered land?...\n",
      "Generated (849 chars): Imagine you're playing in your backyard and you find a brand new patch of grass that no one has ever...\n",
      "\n",
      "================================================================================\n",
      "API test complete!\n"
     ]
    }
   ],
   "source": [
    "# Test with sample questions\n",
    "print(f\"Testing API with {test_sample_size} sample questions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_questions = unique_questions.head(test_sample_size)\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    question = row['title']\n",
    "    print(f\"\\n[Test {idx + 1}] {question[:70]}...\")\n",
    "    answer = generate_gemini_answer(question)\n",
    "    \n",
    "    if answer.startswith(\"ERROR\"):\n",
    "        print(f\"{answer}\")\n",
    "    else:\n",
    "        print(f\"Generated ({len(answer)} chars): {answer[:100]}...\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"API test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d5df2",
   "metadata": {},
   "source": [
    "## 6. Generate LLM Answer for Each Unique Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc9657fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 4000 LLM answers with 6 concurrent workers...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   9%|▉         | 378/4000 [05:10<1:14:50,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempt 1 failed: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  39%|███▉      | 1552/4000 [20:56<34:46,  1.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempt 1 failed: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  49%|████▉     | 1954/4000 [26:15<24:22,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempt 1 failed: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 4000/4000 [53:10<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Generated 4000 / 4000 LLM answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate one LLM answer per unique question using parallel processing\n",
    "llm_answers_map = {}  # Map from q_id to llm_answer\n",
    "\n",
    "questions_to_process = unique_questions_filtered.head(min(num_questions_to_generate, len(unique_questions_filtered)))\n",
    "print(f\"Generating {len(questions_to_process)} LLM answers with {max_workers} concurrent workers...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def process_single_question(row):\n",
    "    \"\"\"\n",
    "    Process a single question and return (q_id, answer) tuple.\n",
    "    \"\"\"\n",
    "    q_id = row['q_id']\n",
    "    question = row['title']\n",
    "    \n",
    "    try:\n",
    "        llm_answer = generate_gemini_answer(question)\n",
    "        \n",
    "        if not llm_answer.startswith(\"ERROR\"):\n",
    "            return (q_id, llm_answer, True)\n",
    "        else:\n",
    "            return (q_id, llm_answer[:80], False)\n",
    "    except Exception as e:\n",
    "        return (q_id, f\"ERROR: {str(e)[:80]}\", False)\n",
    "\n",
    "# Check if there are questions to process\n",
    "if len(questions_to_process) == 0:\n",
    "    print(\"No questions to process. All questions have been generated already.\")\n",
    "else:\n",
    "    # Use ThreadPoolExecutor for concurrent API calls\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = {executor.submit(process_single_question, row): idx \n",
    "                   for idx, row in questions_to_process.iterrows()}\n",
    "        \n",
    "        # Process completed futures with progress bar\n",
    "        with tqdm(total=len(futures), desc=\"Generating answers\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                q_id, result, success = future.result()\n",
    "                \n",
    "                if success:\n",
    "                    llm_answers_map[q_id] = result\n",
    "                else:\n",
    "                    print(f\"\\nSkipped {q_id}: {result}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Rate limiting: brief delay after each completion\n",
    "                time.sleep(delay_between_api_calls / max_workers)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Generated {len(llm_answers_map)} / {len(questions_to_process)} LLM answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ba1ef",
   "metadata": {},
   "source": [
    "## 7. Create LLM Answers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68c69117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new rows: 4000\n",
      "Unique questions: 4000\n",
      "Columns: ['q_id', 'title', 'text', 'source']\n"
     ]
    }
   ],
   "source": [
    "# Create LLM answers dataset\n",
    "llm_rows_list = []\n",
    "\n",
    "for q_id, llm_answer in llm_answers_map.items():\n",
    "    # Get the question info\n",
    "    question_info = unique_questions[unique_questions['q_id'] == q_id].iloc[0]\n",
    "    \n",
    "    llm_rows_list.append({\n",
    "        'q_id': q_id,\n",
    "        'title': question_info['title'],\n",
    "        'text': llm_answer,\n",
    "        'source': 'gemini'\n",
    "    })\n",
    "\n",
    "df_llm = pd.DataFrame(llm_rows_list).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(f\"Total new rows: {len(df_llm)}\")\n",
    "print(f\"Unique questions: {df_llm['q_id'].nunique()}\")\n",
    "print(f\"Columns: {list(df_llm.columns)}\")\n",
    "\n",
    "if len(df_llm) == 0:\n",
    "    print(\"\\nNo new answers generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a52200db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample LLM answers:\n",
      "================================================================================\n",
      "\n",
      "[1] Question: Why we have a dominant hand/foot?...\n",
      "    Answer: Imagine you have two best friends, but you play with one friend a *tiny* bit more often. That friend...\n",
      "    Source: gemini\n",
      "\n",
      "[2] Question: Why are the flags on the moon only faded from the sun, and not entirel...\n",
      "    Answer: Imagine you have a favorite toy that you leave outside in the backyard for a very, very long time. T...\n",
      "    Source: gemini\n",
      "\n",
      "[3] Question: How did we discover planets that are so far away?...\n",
      "    Answer: Imagine you have a super-duper strong flashlight, but it's really, really far away. You can see its ...\n",
      "    Source: gemini\n"
     ]
    }
   ],
   "source": [
    "# Show sample rows\n",
    "print(\"\\nSample LLM answers:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in df_llm.head(3).iterrows():\n",
    "    print(f\"\\n[{idx + 1}] Question: {row['title'][:70]}...\")\n",
    "    print(f\"    Answer: {row['text'][:100]}...\")\n",
    "    print(f\"    Source: {row['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255dc23",
   "metadata": {},
   "source": [
    "## 8. Save LLM Answers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "314c2bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to: gemini-output\\eli5_gemini_answers_20260216_021315.csv\n",
      "Parquet saved to: gemini-output\\eli5_gemini_answers_20260216_021315.parquet\n",
      "Metadata saved to: gemini-output\\metadata_20260216_021315.json\n",
      "Generated 4000 new LLM answers\n",
      "Output folder: gemini-output\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_folder = \"gemini-output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Only save if there are new answers generated\n",
    "if len(df_llm) > 0:\n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_filename = os.path.join(output_folder, f\"eli5_gemini_answers_{timestamp}.csv\")\n",
    "    df_llm.to_csv(csv_filename, index=False)\n",
    "    print(f\"CSV saved to: {csv_filename}\")\n",
    "\n",
    "    # Save as Parquet\n",
    "    parquet_filename = os.path.join(output_folder, f\"eli5_gemini_answers_{timestamp}.parquet\")\n",
    "    df_llm.to_parquet(parquet_filename, index=False)\n",
    "    print(f\"Parquet saved to: {parquet_filename}\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_rows': int(len(df_llm)),\n",
    "        'unique_questions': int(df_llm['q_id'].nunique()),\n",
    "        'gemini_answers': int(len(df_llm)),\n",
    "        'llm_model': gemini_model,\n",
    "        'max_workers': max_workers,\n",
    "        'columns': list(df_llm.columns)\n",
    "    }\n",
    "\n",
    "    metadata_filename = os.path.join(output_folder, f\"metadata_{timestamp}.json\")\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Metadata saved to: {metadata_filename}\")\n",
    "    print(f\"Generated {len(df_llm)} new LLM answers\")\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "else:\n",
    "    print(\"All questions have already been processed in previous runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126a476",
   "metadata": {},
   "source": [
    "## 9. Merge all generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8046b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - eli5_gemini_answers_20260215_230432.csv\n",
      "  - eli5_gemini_answers_20260216_021315.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge all CSVs under gemini folder\n",
    "output_folder = \"gemini-output\"\n",
    "\n",
    "# Find all CSV files in the output folder\n",
    "csv_pattern = os.path.join(output_folder, \"eli5_gemini_answers_*.csv\")\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"No CSV files found in {output_folder}\")\n",
    "else:\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "    \n",
    "    # Load and concatenate all CSV files\n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_file}: {str(e)}\")\n",
    "    \n",
    "    if dfs:\n",
    "        # Merge all dataframes\n",
    "        df_merged = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates based on q_id (keep first occurrence)\n",
    "        df_merged_unique = df_merged.drop_duplicates(subset=['q_id'], keep='first')\n",
    "        \n",
    "        # Save merged file\n",
    "        merged_filename = os.path.join(output_folder, \"eli5_gemini_answers_merged.csv\")\n",
    "        df_merged_unique.to_csv(merged_filename, index=False)\n",
    "        \n",
    "        # Save as parquet\n",
    "        merged_parquet = os.path.join(output_folder, \"eli5_gemini_answers_merged.parquet\")\n",
    "        df_merged_unique.to_parquet(merged_parquet, index=False)\n",
    "    else:\n",
    "        print(\"No data loaded to merge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
