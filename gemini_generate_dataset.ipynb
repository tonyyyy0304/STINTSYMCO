{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d000c52",
   "metadata": {},
   "source": [
    "# ELI5 Dataset - Gemini Answer Generation\n",
    "\n",
    "This notebook processes the ELI5 (Explain Like I'm 5) dataset and generates LLM answers using Google's Gemini.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: HuggingFace dataset `rexarski/eli5_category`\n",
    "- **Period**: January 2017 - June 2021\n",
    "- **Content**: Human-written questions and answers from the ELI5 subreddit\n",
    "- **Purpose**: Generate one LLM answer per unique question and merge with human responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8dbfc",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "\n",
    "**Workflow & Integration:**\n",
    "1. This notebook: Load dataset → Test API → Generate Gemini answers → Merge with human answers\n",
    "2. Model: gemini-2.5-flash\n",
    "3. Each unique question gets ONE LLM answer replicated across all human responses\n",
    "4. **To combine with OpenAI:**\n",
    "   - Run `openai_generate_dataset_clean.ipynb` first (generates human + chatgpt answers)\n",
    "   - Run this notebook (generates human + gemini answers)\n",
    "   - Use the merge function in Cell 10 to combine both without duplicating human answers\n",
    "   - Final dataset: human + chatgpt + gemini sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eec58d",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas numpy datasets\n",
    "# !pip install google-generativeai\n",
    "# !pip install gdown matplotlib seaborn tqdm\n",
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d88e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Gemini API\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3dcf0",
   "metadata": {},
   "source": [
    "## 2. Set Up Gemini API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"✓ Gemini API key configured successfully\")\n",
    "else:\n",
    "    print(\"✗ Warning: Gemini API key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf2b3b",
   "metadata": {},
   "source": [
    "## 3. Load the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7165f",
   "metadata": {},
   "source": [
    "## Configuration - Set Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6683be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions_to_generate = 10\n",
    "delay_between_api_calls = 1\n",
    "test_sample_size = 2\n",
    "\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Questions to generate: {num_questions_to_generate}\")\n",
    "print(f\"  Delay between calls: {delay_between_api_calls}s\")\n",
    "print(f\"  Test sample size: {test_sample_size}\")\n",
    "print(f\"  Model: {gemini_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./human_data/output/eli5_sample.csv\"\n",
    "\n",
    "df_human = pd.read_csv(path)\n",
    "print(f\"✓ Dataset loaded with {len(df_human)} records\")\n",
    "\n",
    "unique_questions = df_human[['q_id', 'title']].drop_duplicates().reset_index(drop=True)\n",
    "print(f\"✓ Found {len(unique_questions)} unique questions\")\n",
    "print(f\"  Average answers per question: {len(df_human) / len(unique_questions):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc78056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of dataset:\")\n",
    "print(\"=\" * 80)\n",
    "df_human[['q_id', 'title', 'text']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac5c5a",
   "metadata": {},
   "source": [
    "## 4. Define LLM Answer Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0525088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_answer(question, model=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate an ELI5-style answer using Gemini.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        model: Gemini model to use (default: gemini-2.5-flash)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer as string, or error message if failed\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = gemini_model\n",
    "        \n",
    "    if not GEMINI_API_KEY:\n",
    "        return \"ERROR: Gemini API key not configured\"\n",
    "    \n",
    "    prompt = f\"\"\"You are answering questions in the style of the ELI5 (Explain Like I'm 5) subreddit. \n",
    "Provide a clear, simple explanation that a 5-year-old could understand, but still be informative.\n",
    "Keep everything as one block of text.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model_instance = genai.GenerativeModel(model)\n",
    "            response = model_instance.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=0.7,\n",
    "                    max_output_tokens=1000,\n",
    "                )\n",
    "            )\n",
    "            return response.text.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57ad7f",
   "metadata": {},
   "source": [
    "## 5. Test API with Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ca9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "print(f\"Testing API with {test_sample_size} sample questions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_questions = unique_questions.head(test_sample_size)\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    question = row['title']\n",
    "    print(f\"\\n[Test {idx + 1}] {question[:70]}...\")\n",
    "    answer = generate_gemini_answer(question)\n",
    "    \n",
    "    if answer.startswith(\"ERROR\"):\n",
    "        print(f\"✗ {answer}\")\n",
    "    else:\n",
    "        print(f\"✓ Generated ({len(answer)} chars): {answer[:100]}...\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"API test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d5df2",
   "metadata": {},
   "source": [
    "## 6. Generate LLM Answer for Each Unique Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9657fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one LLM answer per unique question\n",
    "llm_answers_map = {}  # Map from q_id to llm_answer\n",
    "\n",
    "questions_to_process = unique_questions.head(min(num_questions_to_generate, len(unique_questions)))\n",
    "print(f\"Generating {len(questions_to_process)} LLM answers...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with tqdm(total=len(questions_to_process), desc=\"Generating answers\") as pbar:\n",
    "    for idx, row in questions_to_process.iterrows():\n",
    "        q_id = row['q_id']\n",
    "        question = row['title']\n",
    "        \n",
    "        llm_answer = generate_gemini_answer(question)\n",
    "        \n",
    "        if not llm_answer.startswith(\"ERROR\"):\n",
    "            llm_answers_map[q_id] = llm_answer\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            print(f\"\\n✗ Skipped {q_id}: {llm_answer[:80]}\")\n",
    "        \n",
    "        time.sleep(delay_between_api_calls)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"✓ Successfully generated {len(llm_answers_map)} / {len(questions_to_process)} LLM answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ba1ef",
   "metadata": {},
   "source": [
    "## 7. Merge LLM Answers with Human Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c69117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create human answers dataset with source column\n",
    "df_human_labeled = df_human.copy()\n",
    "df_human_labeled['source'] = 'human'\n",
    "\n",
    "# Create LLM answers dataset from the map\n",
    "# Get one row per unique question from the original data\n",
    "unique_rows = df_human.drop_duplicates(subset=['q_id'])[['q_id', 'title']].reset_index(drop=True)\n",
    "\n",
    "# Filter to only questions that have LLM answers\n",
    "llm_rows_list = []\n",
    "for q_id, llm_answer in llm_answers_map.items():\n",
    "    # Get the first row for this q_id to use as template\n",
    "    template_row = df_human[df_human['q_id'] == q_id].iloc[0].copy()\n",
    "    template_row['text'] = llm_answer\n",
    "    template_row['source'] = 'gemini'\n",
    "    llm_rows_list.append(template_row)\n",
    "\n",
    "df_llm_labeled = pd.DataFrame(llm_rows_list).reset_index(drop=True)\n",
    "\n",
    "# Combine human and LLM answers\n",
    "df_combined = pd.concat([df_human_labeled, df_llm_labeled], ignore_index=True)\n",
    "\n",
    "# Shuffle for better mixing\n",
    "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Combined dataset summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {len(df_combined)}\")\n",
    "print(f\"Human answers: {(df_combined['source'] == 'human').sum()}\")\n",
    "print(f\"Gemini answers: {(df_combined['source'] == 'gemini').sum()}\")\n",
    "print(f\"Columns: {list(df_combined.columns)}\")\n",
    "print(f\"\\nDataset shape: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52200db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample rows\n",
    "print(\"\\nSample rows from combined dataset:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for source_type in ['human', 'gemini']:\n",
    "    sample = df_combined[df_combined['source'] == source_type][['q_id', 'title', 'text', 'source']].head(1)\n",
    "    if not sample.empty:\n",
    "        row = sample.iloc[0]\n",
    "        print(f\"\\n{source_type.upper()}:\")\n",
    "        print(f\"  Question: {row['title'][:60]}\")\n",
    "        print(f\"  Answer: {row['text'][:80]}...\")\n",
    "        print(f\"  Source: {row['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255dc23",
   "metadata": {},
   "source": [
    "## 8. Save Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "output_folder = \"gemini-output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = os.path.join(output_folder, f\"eli5_combined_dataset_{timestamp}.csv\")\n",
    "df_combined.to_csv(csv_filename, index=False)\n",
    "print(f\"✓ CSV saved to: {csv_filename}\")\n",
    "\n",
    "# Save as Parquet\n",
    "parquet_filename = os.path.join(output_folder, f\"eli5_combined_dataset_{timestamp}.parquet\")\n",
    "df_combined.to_parquet(parquet_filename, index=False)\n",
    "print(f\"✓ Parquet saved to: {parquet_filename}\")\n",
    "\n",
    "# Save metadata\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_rows': len(df_combined),\n",
    "    'unique_questions': len(unique_questions),\n",
    "    # Add int() wrapper around these sums\n",
    "    'human_answers': int((df_combined['source'] == 'human').sum()),\n",
    "    'chatgpt_answers': int((df_combined['source'] == 'chatgpt').sum()),\n",
    "    'llm_model': 'gpt-4o-mini',\n",
    "    'columns': list(df_combined.columns)\n",
    "}\n",
    "\n",
    "metadata_filename = os.path.join(output_folder, f\"metadata_{timestamp}.json\")\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Metadata saved to: {metadata_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET GENERATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02696e",
   "metadata": {},
   "source": [
    "## 9. Optional: Merge OpenAI and Gemini Outputs\n",
    "\n",
    "If you've also run the OpenAI notebook, use this cell to combine outputs without duplicating human answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Merge OpenAI and Gemini outputs\n",
    "# This prevents duplicate human answers by only combining LLM answers from both models\n",
    "\n",
    "import glob\n",
    "\n",
    "def merge_openai_gemini_outputs(combined_output_folder=\"merged-output\"):\n",
    "    \"\"\"\n",
    "    Combines OpenAI and Gemini outputs into a single dataset.\n",
    "    Keeps human answers from OpenAI output, adds only Gemini LLM answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if both output folders exist\n",
    "    if not os.path.exists(\"openai-output\"):\n",
    "        print(\"✗ OpenAI output folder not found. Run OpenAI notebook first.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(\"gemini-output\"):\n",
    "        print(\"✗ Gemini output folder not found. Run Gemini notebook first.\")\n",
    "        return\n",
    "    \n",
    "    # Get latest files from each folder\n",
    "    openai_files = glob.glob(\"openai-output/eli5_combined_dataset_*.csv\")\n",
    "    gemini_files = glob.glob(\"gemini-output/eli5_combined_dataset_*.csv\")\n",
    "    \n",
    "    if not openai_files or not gemini_files:\n",
    "        print(\"✗ Could not find combined dataset CSV files\")\n",
    "        return\n",
    "    \n",
    "    openai_csv = max(openai_files, key=os.path.getctime)  # Get newest\n",
    "    gemini_csv = max(gemini_files, key=os.path.getctime)\n",
    "    \n",
    "    print(f\"Loading OpenAI dataset: {openai_csv}\")\n",
    "    df_openai = pd.read_csv(openai_csv)\n",
    "    \n",
    "    print(f\"Loading Gemini dataset: {gemini_csv}\")\n",
    "    df_gemini = pd.read_csv(gemini_csv)\n",
    "    \n",
    "    # Keep only human answers from OpenAI (avoid duplicates)\n",
    "    df_human = df_openai[df_openai['source'] == 'human'].copy()\n",
    "    \n",
    "    # Keep only LLM answers from both\n",
    "    df_openai_llm = df_openai[df_openai['source'] == 'chatgpt'].copy()\n",
    "    df_gemini_llm = df_gemini[df_gemini['source'] == 'gemini'].copy()\n",
    "    \n",
    "    # Combine: human + chatgpt + gemini\n",
    "    df_merged = pd.concat([df_human, df_openai_llm, df_gemini_llm], ignore_index=True)\n",
    "    df_merged = df_merged.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Create merged output folder\n",
    "    os.makedirs(combined_output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save merged dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    merged_csv = os.path.join(combined_output_folder, f\"eli5_openai_gemini_merged_{timestamp}.csv\")\n",
    "    df_merged.to_csv(merged_csv, index=False)\n",
    "    \n",
    "    merged_parquet = os.path.join(combined_output_folder, f\"eli5_openai_gemini_merged_{timestamp}.parquet\")\n",
    "    df_merged.to_parquet(merged_parquet, index=False)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_rows': int(len(df_merged)),  # Good practice to cast this too\n",
    "        # Add int() wrapper around these sums\n",
    "        'human_answers': int((df_merged['source'] == 'human').sum()),\n",
    "        'chatgpt_answers': int((df_merged['source'] == 'chatgpt').sum()),\n",
    "        'gemini_answers': int((df_merged['source'] == 'gemini').sum()),\n",
    "        'source_openai': openai_csv,\n",
    "        'source_gemini': gemini_csv,\n",
    "        'columns': list(df_merged.columns)\n",
    "    }\n",
    "    \n",
    "    metadata_file = os.path.join(combined_output_folder, f\"metadata_{timestamp}.json\")\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✓ Merged dataset created successfully!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(df_merged)}\")\n",
    "    print(f\"  - Human answers: {(df_merged['source'] == 'human').sum()}\")\n",
    "    print(f\"  - ChatGPT answers: {(df_merged['source'] == 'chatgpt').sum()}\")\n",
    "    print(f\"  - Gemini answers: {(df_merged['source'] == 'gemini').sum()}\")\n",
    "    print(f\"\\nOutput folder: {combined_output_folder}\")\n",
    "    print(f\"Files saved: {merged_csv}, {merged_parquet}, {metadata_file}\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# Run merge if both datasets exist\n",
    "# Uncomment the line below to merge OpenAI and Gemini outputs:\n",
    "df_final_merged = merge_openai_gemini_outputs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
