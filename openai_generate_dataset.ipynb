{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f1b0d5",
   "metadata": {},
   "source": [
    "# ELI5 Dataset - ChatGPT Answer Generation\n",
    "\n",
    "This notebook processes the ELI5 (Explain Like I'm 5) dataset and generates LLM answers using OpenAI's ChatGPT.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: HuggingFace dataset `rexarski/eli5_category`\n",
    "- **Period**: January 2017 - June 2021\n",
    "- **Content**: Human-written questions and answers from the ELI5 subreddit\n",
    "- **Purpose**: Generate one LLM answer per unique question and merge with human responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e67a35",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "\n",
    "**Workflow & Integration:**\n",
    "1. This notebook: Load dataset → Test API → Generate ChatGPT answers → Merge with human answers\n",
    "2. Model: gpt-4o-mini\n",
    "3. Each unique question gets ONE LLM answer replicated across all human responses\n",
    "4. **To combine with Gemini:**\n",
    "   - Run this notebook first (generates human + chatgpt answers)\n",
    "   - Run `gemini_generate_dataset_clean.ipynb` (generates gemini answers)\n",
    "   - Use the merge function in Cell 10 to combine both without duplicating human answers\n",
    "   - Final dataset: human + chatgpt + gemini sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e78e1",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b884cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas numpy datasets\n",
    "# !pip install openai\n",
    "# !pip install gdown matplotlib seaborn tqdm\n",
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# OpenAI API\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1093da",
   "metadata": {},
   "source": [
    "## 2. Set Up OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"OpenAI API key configured successfully\")\n",
    "else:\n",
    "    print(\"OpenAI API key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97910b0a",
   "metadata": {},
   "source": [
    "## 3. Load the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0214ab",
   "metadata": {},
   "source": [
    "## Configuration - Set Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions_to_generate = 1000\n",
    "delay_between_api_calls = 0.7\n",
    "test_sample_size = 2\n",
    "max_workers = 6  # Number of concurrent API calls\n",
    "\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Questions to generate: {num_questions_to_generate}\")\n",
    "print(f\"  Delay between calls: {delay_between_api_calls}s\")\n",
    "print(f\"  Max concurrent workers: {max_workers}\")\n",
    "print(f\"  Test sample size: {test_sample_size}\")\n",
    "print(f\"  Model: {openai_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ea40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./human_data/output/eli5_cleaned.csv\"\n",
    "\n",
    "df_human = pd.read_csv(path)\n",
    "print(f\"✓ Dataset loaded with {len(df_human)} records\")\n",
    "\n",
    "# Get unique questions\n",
    "unique_questions = df_human[['q_id', 'title']].drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Found {len(unique_questions)} unique questions\")\n",
    "print(f\"Average answers per question: {len(df_human) / len(unique_questions):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8631d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of dataset:\")\n",
    "print(\"=\" * 80)\n",
    "df_human[['q_id', 'title', 'text']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f185b30",
   "metadata": {},
   "source": [
    "## 4. Define LLM Answer Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatgpt_answer(question, model=\"gpt-4o-mini\", max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate an ELI5-style answer using ChatGPT.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        model: OpenAI model to use (default: gpt-4o-mini)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer as string, or error message if failed\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"ERROR: OpenAI API key not configured\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are answering questions in the style of the ELI5 (Explain Like I'm 5) subreddit. \n",
    "Provide a clear, simple explanation that a 5-year-old could understand, but still be informative.\n",
    "Keep everything as one block of text.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ec9c",
   "metadata": {},
   "source": [
    "## 5. Test API with Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "print(f\"Testing API with {test_sample_size} sample questions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_questions = unique_questions.head(test_sample_size)\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    question = row['title']\n",
    "    print(f\"\\n[Test {idx + 1}] {question[:70]}...\")\n",
    "    answer = generate_chatgpt_answer(question)\n",
    "    \n",
    "    if answer.startswith(\"ERROR\"):\n",
    "        print(f\"{answer}\")\n",
    "    else:\n",
    "        print(f\"Generated ({len(answer)} chars): {answer[:100]}...\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"API test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2fb95",
   "metadata": {},
   "source": [
    "## 6. Generate LLM Answer for Each Unique Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one LLM answer per unique question using parallel processing\n",
    "llm_answers_map = {}  # Map from q_id to llm_answer\n",
    "\n",
    "questions_to_process = unique_questions.head(min(num_questions_to_generate, len(unique_questions)))\n",
    "\n",
    "def process_single_question(row):\n",
    "    \"\"\"\n",
    "    Process a single question and return (q_id, answer) tuple.\n",
    "    \"\"\"\n",
    "    q_id = row['q_id']\n",
    "    question = row['title']\n",
    "    \n",
    "    try:\n",
    "        llm_answer = generate_chatgpt_answer(question)\n",
    "        \n",
    "        if not llm_answer.startswith(\"ERROR\"):\n",
    "            return (q_id, llm_answer, True)\n",
    "        else:\n",
    "            return (q_id, llm_answer[:80], False)\n",
    "    except Exception as e:\n",
    "        return (q_id, f\"ERROR: {str(e)[:80]}\", False)\n",
    "\n",
    "# Use ThreadPoolExecutor for concurrent API calls\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(process_single_question, row): idx \n",
    "               for idx, row in questions_to_process.iterrows()}\n",
    "    \n",
    "    # Process completed futures with progress bar\n",
    "    with tqdm(total=len(futures), desc=\"Generating answers\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            q_id, result, success = future.result()\n",
    "            \n",
    "            if success:\n",
    "                llm_answers_map[q_id] = result\n",
    "            else:\n",
    "                print(f\"\\nSkipped {q_id}: {result}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Rate limiting: brief delay after each completion\n",
    "            time.sleep(delay_between_api_calls / max_workers)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"Generated {len(llm_answers_map)} / {len(questions_to_process)} LLM answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d594c63",
   "metadata": {},
   "source": [
    "## 7. Create LLM Answers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM answers dataset\n",
    "llm_rows_list = []\n",
    "\n",
    "for q_id, llm_answer in llm_answers_map.items():\n",
    "    # Get the question info\n",
    "    question_info = unique_questions[unique_questions['q_id'] == q_id].iloc[0]\n",
    "    \n",
    "    llm_rows_list.append({\n",
    "        'q_id': q_id,\n",
    "        'title': question_info['title'],\n",
    "        'text': llm_answer,\n",
    "        'source': 'chatgpt'\n",
    "    })\n",
    "\n",
    "df_llm = pd.DataFrame(llm_rows_list).reset_index(drop=True)\n",
    "\n",
    "print(\"LLM Answers Dataset Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {len(df_llm)}\")\n",
    "print(f\"Unique questions: {df_llm['q_id'].nunique()}\")\n",
    "print(f\"Columns: {list(df_llm.columns)}\")\n",
    "print(f\"\\nDataset shape: {df_llm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample rows\n",
    "print(\"\\nSample LLM answers:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in df_llm.head(3).iterrows():\n",
    "    print(f\"\\n[{idx + 1}] Question: {row['title'][:70]}...\")\n",
    "    print(f\"    Answer: {row['text'][:100]}...\")\n",
    "    print(f\"    Source: {row['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00494fa2",
   "metadata": {},
   "source": [
    "## 8. Save LLM Answers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbb15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "output_folder = \"openai-output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = os.path.join(output_folder, f\"eli5_chatgpt_answers_{timestamp}.csv\")\n",
    "df_llm.to_csv(csv_filename, index=False)\n",
    "print(f\"CSV saved to: {csv_filename}\")\n",
    "\n",
    "# Save as Parquet\n",
    "parquet_filename = os.path.join(output_folder, f\"eli5_chatgpt_answers_{timestamp}.parquet\")\n",
    "df_llm.to_parquet(parquet_filename, index=False)\n",
    "print(f\"Parquet saved to: {parquet_filename}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_rows': int(len(df_llm)),\n",
    "    'unique_questions': int(df_llm['q_id'].nunique()),\n",
    "    'chatgpt_answers': int(len(df_llm)),\n",
    "    'llm_model': openai_model,\n",
    "    'max_workers': max_workers,\n",
    "    'columns': list(df_llm.columns)\n",
    "}\n",
    "\n",
    "metadata_filename = os.path.join(output_folder, f\"metadata_{timestamp}.json\")\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Metadata saved to: {metadata_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
