{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74321669",
   "metadata": {},
   "source": [
    "# ELI5 Dataset - ChatGPT Answer Generation\n",
    "\n",
    "This notebook processes the ELI5 (Explain Like I'm 5) dataset and generates answers using OpenAI's ChatGPT.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: HuggingFace dataset `rexarski/eli5_category`\n",
    "- **Period**: January 2017 - June 2021\n",
    "- **Content**: Human-written questions and answers from the ELI5 subreddit\n",
    "- **Purpose**: Expand dataset with ChatGPT-generated answers for comparison and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea435bcb",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "1. Finalize how what columns to pick for the final dataset\n",
    "    * Current: drop score & subreddit column ONLY\n",
    "2. Finalize what models to use\n",
    "    * Current: gpt-4o-mini\n",
    "3. The current ChatGPT df generation only saves the successful attempts to df_chatgpt (i.e. sample size = 10, but 2 fail, final df length is 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dc263",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy datasets\n",
    "!pip install openai\n",
    "!pip install gdown matplotlib seaborn tqdm\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "# OpenAI API\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010b818",
   "metadata": {},
   "source": [
    "## 2. Set Up OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"OpenAI API key configured successfully\")\n",
    "else:\n",
    "    print(\"Warning: OpenAI API key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bd9b3",
   "metadata": {},
   "source": [
    "## 3. Load the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261dfb1",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "You must run the scripts in the human_data folder to get the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./human_data/output/eli5_combined.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "print(f\"Dataset loaded with {len(df)} records\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689c076",
   "metadata": {},
   "source": [
    "## 4. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf11cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST FEW ROWS\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b463f4c",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(df_clean)} rows\")\n",
    "\n",
    "# 1. Remove duplicates\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"After removing duplicates: {len(df_clean)} rows\")\n",
    "\n",
    "# 2. Filter by text length (keep only answers with <= 1000 characters)\n",
    "# Justification: Mean is approx. 600 and Median is approx. 400. \n",
    "# Highest is around 9000 and token limit is 1000\n",
    "df_clean = df_clean[df_clean['text'].str.len() <= 1000]\n",
    "print(f\"After filtering text length: {len(df_clean)} rows\")\n",
    "\n",
    "# 3. Drop unnecessary columns (score && subreddit)\n",
    "df_clean = df_clean.drop(columns=['score', 'subreddit'], errors='ignore')\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset size: {len(df_clean)} rows\")\n",
    "print(f\"Removed: {len(df) - len(df_clean)} rows ({((len(df) - len(df_clean)) / len(df) * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da777ce",
   "metadata": {},
   "source": [
    "## 6. Load questions to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997716e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_clean['title'].tolist()\n",
    "\n",
    "print(\"Question num: \"  + str(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527fa6a0",
   "metadata": {},
   "source": [
    "## 7. Generate Answers with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatgpt_answer(question, model=\"gpt-4o-mini\", max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate an ELI5-style answer using ChatGPT.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        model: OpenAI model to use (default: gpt-4o-mini)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer as string, or error message if failed\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"ERROR: OpenAI API key not configured\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are answering questions in the style of the ELI5 (Explain Like I'm 5) subreddit. \n",
    "Provide a clear, simple explanation that a 5-year-old could understand, but still be informative.\n",
    "Keep everything as one block of text.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test the function\n",
    "print(\"TEST\")\n",
    "test_question = \"Why is the sky blue?\"\n",
    "test_answer = generate_chatgpt_answer(test_question)\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Answer: {test_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d68a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ChatGPT answers for the dataset\n",
    "\n",
    "def batch_generate_chatgpt(df, questions, sample_size=None, delay=1):\n",
    "    # Determine which indices to process\n",
    "    if sample_size is None:\n",
    "        target_count = len(questions)\n",
    "    else:\n",
    "        target_count = sample_size\n",
    "    \n",
    "    successful_rows = []\n",
    "    successful_indices = []  # Track which rows succeeded\n",
    "    idx = 0\n",
    "    \n",
    "    # Keep generating until we have enough successful answers\n",
    "    with tqdm(total=target_count, desc=\"Generating ChatGPT answers\") as pbar:\n",
    "        while len(successful_rows) < target_count and idx < len(questions):\n",
    "            question = questions[idx]\n",
    "            answer = generate_chatgpt_answer(question)\n",
    "            \n",
    "            # Only include if answer doesn't start with ERROR\n",
    "            if not answer.startswith(\"ERROR\"):\n",
    "                row = df.iloc[idx].copy()\n",
    "                row['text'] = answer\n",
    "                successful_rows.append(row)\n",
    "                successful_indices.append(idx)\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                print(f\"\\nSkipping row {idx} due to error\")\n",
    "            \n",
    "            idx += 1\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    df_chatgpt = pd.DataFrame(successful_rows).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nSuccessfully generated {len(df_chatgpt)} answers\")\n",
    "    if idx >= len(questions) and len(df_chatgpt) < target_count:\n",
    "        print(f\"Warning: Only got {len(df_chatgpt)} successful answers out of {target_count} requested\")\n",
    "    \n",
    "    return df_chatgpt, successful_indices\n",
    "\n",
    "# CHANGE SAMPLE_SIZE\n",
    "df_chatgpt, chatgpt_indices = batch_generate_chatgpt(df_clean, questions, sample_size, delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chatgpt.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fe12c",
   "metadata": {},
   "source": [
    "## 7. Compare Human vs ChatGPT Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bda0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate answer lengths for both human and ChatGPT datasets\n",
    "def calculate_answer_stats(df_human, df_chatgpt):\n",
    "    \"\"\"Calculate statistics for human and ChatGPT-generated answers.\"\"\"\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # Human stats (from df_clean)\n",
    "    if 'text' in df_human.columns:\n",
    "        human_lengths = df_human['text'].astype(str).str.len()\n",
    "        stats['human'] = {\n",
    "            'mean_length': human_lengths.mean(),\n",
    "            'median_length': human_lengths.median(),\n",
    "            'max_length': human_lengths.max(),\n",
    "            'min_length': human_lengths.min()\n",
    "        }\n",
    "    \n",
    "    # ChatGPT stats (from df_chatgpt)\n",
    "    if 'text' in df_chatgpt.columns:\n",
    "        chatgpt_lengths = df_chatgpt['text'].astype(str).str.len()\n",
    "        stats['chatgpt'] = {\n",
    "            'mean_length': chatgpt_lengths.mean(),\n",
    "            'median_length': chatgpt_lengths.median(),\n",
    "            'max_length': chatgpt_lengths.max(),\n",
    "            'min_length': chatgpt_lengths.min()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate stats\n",
    "stats = calculate_answer_stats(df_clean, df_chatgpt)\n",
    "\n",
    "# Display stats\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER LENGTH STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "for source, metrics in stats.items():\n",
    "    print(f\"\\n{source.upper()}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize answer length comparison\n",
    "\n",
    "def plot_answer_length_comparison(df_human, df_chatgpt):\n",
    "    \"\"\"Create visualization comparing answer lengths across different sources.\"\"\"\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    \n",
    "    if 'text' in df_human.columns:\n",
    "        plot_data.append({\n",
    "            'source': 'Human',\n",
    "            'lengths': df_human['text'].astype(str).str.len().tolist()\n",
    "        })\n",
    "    \n",
    "    if 'text' in df_chatgpt.columns:\n",
    "        plot_data.append({\n",
    "            'source': 'ChatGPT',\n",
    "            'lengths': df_chatgpt['text'].astype(str).str.len().tolist()\n",
    "        })\n",
    "    \n",
    "    if not plot_data:\n",
    "        print(\"No data available for plotting yet.\")\n",
    "        return\n",
    "    \n",
    "    # Create box plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    sources = [d['source'] for d in plot_data]\n",
    "    lengths = [d['lengths'] for d in plot_data]\n",
    "    \n",
    "    axes[0].boxplot(lengths, labels=sources)\n",
    "    axes[0].set_title('Answer Length Distribution by Source')\n",
    "    axes[0].set_ylabel('Answer Length (characters)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bar chart of mean lengths\n",
    "    mean_lengths = [np.mean(d['lengths']) for d in plot_data]\n",
    "    axes[1].bar(sources, mean_lengths, color=['blue', 'green'][:len(sources)])\n",
    "    axes[1].set_title('Mean Answer Length by Source')\n",
    "    axes[1].set_ylabel('Mean Length (characters)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "plot_answer_length_comparison(df_clean, df_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c086f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample comparison - View actual answers side by side\n",
    "def compare_answers(df_human, df_chatgpt, index=0, width=100):\n",
    "    \"\"\"Display a side-by-side comparison of answers for a specific question.\"\"\"\n",
    "    \n",
    "    row_human = df_human.iloc[index]\n",
    "    row_chatgpt = df_chatgpt.iloc[index]\n",
    "    \n",
    "    print(f\"QUESTION: {row_human['title']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"-\" * width)\n",
    "    print(\"HUMAN ANSWER\")\n",
    "    print(\"-\" * width)\n",
    "    wrapped_text = textwrap.fill(str(row_human['text']), width=width)\n",
    "    print(wrapped_text)\n",
    "    print()\n",
    "    \n",
    "    print(\"-\" * width)\n",
    "    print(\"CHATGPT ANSWER\")\n",
    "    print(\"-\" * width)\n",
    "    wrapped_answer = textwrap.fill(str(row_chatgpt['text']), width=width)\n",
    "    print(wrapped_answer)\n",
    "    print()\n",
    "\n",
    "# Compare first answer\n",
    "compare_answers(df_clean, df_chatgpt, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce8190",
   "metadata": {},
   "source": [
    "## 8. Save Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c77eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enhanced dataset with ALL human and ChatGPT answers combined and shuffled\n",
    "\n",
    "# Prepare human answers dataset\n",
    "df_human_labeled = df_clean.copy()\n",
    "df_human_labeled['answer_source'] = 'human'\n",
    "\n",
    "# Prepare ChatGPT answers dataset\n",
    "df_chatgpt_labeled = df_chatgpt.copy()\n",
    "df_chatgpt_labeled['answer_source'] = 'chatgpt'\n",
    "\n",
    "# Combine ALL answers (human + ChatGPT)\n",
    "df_combined = pd.concat([df_human_labeled, df_chatgpt_labeled], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset created:\")\n",
    "print(f\"  Total rows: {len(df_combined)}\")\n",
    "print(f\"  Human answers: {(df_combined['answer_source'] == 'human').sum()} ({(df_combined['answer_source'] == 'human').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"  ChatGPT answers: {(df_combined['answer_source'] == 'chatgpt').sum()} ({(df_combined['answer_source'] == 'chatgpt').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"  Columns: {list(df_combined.columns)}\")\n",
    "print()\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "output_folder = \"openai-output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = os.path.join(output_folder, f\"eli5_combined_{timestamp}.csv\")\n",
    "\n",
    "# Save to CSV\n",
    "df_combined.to_csv(output_filename, index=False)\n",
    "print(f\"Combined dataset saved to: {output_filename}\")\n",
    "\n",
    "# Optional: Save as parquet for better compression and faster loading\n",
    "parquet_filename = os.path.join(output_folder, f\"eli5_combined_{timestamp}.parquet\")\n",
    "df_combined.to_parquet(parquet_filename, index=False)\n",
    "print(f\"Combined dataset saved to: {parquet_filename}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_filename = os.path.join(output_folder, f\"eli5_summary_{timestamp}.json\")\n",
    "summary = {\n",
    "    'total_rows': len(df_combined),\n",
    "    'columns': list(df_combined.columns),\n",
    "    'human_answers': int((df_combined['answer_source'] == 'human').sum()),\n",
    "    'chatgpt_answers': int((df_combined['answer_source'] == 'chatgpt').sum()),\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary statistics saved to: {summary_filename}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET COMBINATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
