{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aaf271f",
   "metadata": {},
   "source": [
    "# ELI5 Dataset - DeepSeek Answer Generation\n",
    "\n",
    "This notebook processes the ELI5 (Explain Like I'm 5) dataset and generates LLM answers using DeepSeek.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: HuggingFace dataset `rexarski/eli5_category`\n",
    "- **Period**: January 2017 - June 2021\n",
    "- **Content**: Human-written questions and answers from the ELI5 subreddit\n",
    "- **Purpose**: Generate one LLM answer per unique question and merge with human responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff8a16",
   "metadata": {},
   "source": [
    "####  **<span style=\"color:red\">IMPORTANT: <span>**\n",
    "\n",
    "**Workflow & Integration:**\n",
    "1. This notebook: Load dataset → Test API → Generate DeepSeek answers → Merge with human answers\n",
    "2. Model: deepseek-chat\n",
    "3. Each unique question gets ONE LLM answer replicated across all human responses\n",
    "4. **To combine with other models:**\n",
    "   - Run `openai_generate_dataset.ipynb` first (generates human + chatgpt answers)\n",
    "   - Run `gemini_generate_dataset.ipynb` (generates gemini answers)\n",
    "   - Run this notebook (generates deepseek answers)\n",
    "   - Use the merge function to combine all without duplicating human answers\n",
    "   - Final dataset: human + chatgpt + gemini + deepseek sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda80cc9",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas numpy datasets\n",
    "# !pip install openai\n",
    "# !pip install gdown matplotlib seaborn tqdm\n",
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8938d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# OpenAI-compatible API (for DeepSeek)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbce61b",
   "metadata": {},
   "source": [
    "## 2. Set Up DeepSeek API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d20de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API key configured successfully\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "if DEEPSEEK_API_KEY:\n",
    "    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "    print(\"DeepSeek API key configured successfully\")\n",
    "else:\n",
    "    print(\"DeepSeek API key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664adc6",
   "metadata": {},
   "source": [
    "## 3. Load the ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41ff93",
   "metadata": {},
   "source": [
    "## Configuration - Set Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18f9646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Questions to generate: 5000\n",
      "  Delay between calls: 1s\n",
      "  Max concurrent workers: 6\n",
      "  Test sample size: 2\n",
      "  Model: deepseek-chat\n"
     ]
    }
   ],
   "source": [
    "num_questions_to_generate = 5000\n",
    "delay_between_api_calls = 1\n",
    "test_sample_size = 2\n",
    "max_workers = 6  # Number of concurrent API calls\n",
    "\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Questions to generate: {num_questions_to_generate}\")\n",
    "print(f\"  Delay between calls: {delay_between_api_calls}s\")\n",
    "print(f\"  Max concurrent workers: {max_workers}\")\n",
    "print(f\"  Test sample size: {test_sample_size}\")\n",
    "print(f\"  Model: {deepseek_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21452337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 228637 records\n",
      "Found 93893 unique questions\n",
      "Average answers per question: 2.4\n"
     ]
    }
   ],
   "source": [
    "path = \"./human_data/output/eli5_cleaned.csv\"\n",
    "\n",
    "df_human = pd.read_csv(path)\n",
    "print(f\"Dataset loaded with {len(df_human)} records\")\n",
    "\n",
    "# Get unique questions\n",
    "unique_questions = df_human[['q_id', 'title']].drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Found {len(unique_questions)} unique questions\")\n",
    "print(f\"Average answers per question: {len(df_human) / len(unique_questions):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc4a24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of dataset:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5lchat</td>\n",
       "      <td>Why there was a 'leap second' added to the end of 2016?</td>\n",
       "      <td>the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5lchat</td>\n",
       "      <td>Why there was a 'leap second' added to the end of 2016?</td>\n",
       "      <td>The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5lchat</td>\n",
       "      <td>Why there was a 'leap second' added to the end of 2016?</td>\n",
       "      <td>Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.] In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_id                                                    title  \\\n",
       "0  5lchat  Why there was a 'leap second' added to the end of 2016?   \n",
       "1  5lchat  Why there was a 'leap second' added to the end of 2016?   \n",
       "2  5lchat  Why there was a 'leap second' added to the end of 2016?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.  \n",
       "1  The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.] In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of dataset:\")\n",
    "print(\"=\" * 80)\n",
    "df_human[['q_id', 'title', 'text']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c46b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder 'deepseek-output' does not exist - starting fresh\n",
      "Total unique questions: 93893\n",
      "Already generated: 0\n",
      "Remaining to generate: 93893\n"
     ]
    }
   ],
   "source": [
    "# Load existing generated answers from output folder to avoid regenerating\n",
    "output_folder = \"deepseek-output\"\n",
    "existing_q_ids = set()\n",
    "\n",
    "if os.path.exists(output_folder):\n",
    "    # Find all CSV files in the output folder\n",
    "    csv_files = [f for f in os.listdir(output_folder) if f.startswith('eli5_deepseek_answers_') and f.endswith('.csv')]\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"Found {len(csv_files)} existing output file(s):\")\n",
    "        for csv_file in csv_files:\n",
    "            filepath = os.path.join(output_folder, csv_file)\n",
    "            try:\n",
    "                df_existing = pd.read_csv(filepath)\n",
    "                existing_q_ids.update(df_existing['q_id'].unique())\n",
    "                print(f\"  - {csv_file}: {df_existing['q_id'].nunique()} questions\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - {csv_file}: Error reading file - {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nTotal unique questions already generated: {len(existing_q_ids)}\")\n",
    "    else:\n",
    "        print(\"No existing output files found\")\n",
    "else:\n",
    "    print(f\"Output folder '{output_folder}' does not exist - starting fresh\")\n",
    "\n",
    "# Filter out already-processed questions\n",
    "unique_questions_filtered = unique_questions[~unique_questions['q_id'].isin(existing_q_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total unique questions: {len(unique_questions)}\")\n",
    "print(f\"Already generated: {len(existing_q_ids)}\")\n",
    "print(f\"Remaining to generate: {len(unique_questions_filtered)}\")\n",
    "\n",
    "if len(unique_questions_filtered) == 0:\n",
    "    print(\"\\nAll questions have already been processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbbe400",
   "metadata": {},
   "source": [
    "## 4. Define LLM Answer Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3693cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deepseek_answer(question, model=\"deepseek-chat\", max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate an ELI5-style answer using DeepSeek.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        model: DeepSeek model to use (default: deepseek-chat)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer as string, or error message if failed\n",
    "    \"\"\"\n",
    "    if not DEEPSEEK_API_KEY:\n",
    "        return \"ERROR: DeepSeek API key not configured\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are answering questions in the style of the ELI5 (Explain Like I'm 5) subreddit. \n",
    "Provide a clear, simple explanation that a 5-year-old could understand, but still be informative.\n",
    "Keep everything as one block of text.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98e2d3",
   "metadata": {},
   "source": [
    "## 5. Test API with Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2b5bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API with 2 sample questions...\n",
      "================================================================================\n",
      "\n",
      "[Test 1] Why there was a 'leap second' added to the end of 2016?...\n",
      "Generated (641 chars): Think of Earth like a spinning top that's starting to wobble and slow down just a tiny bit over a ve...\n",
      "\n",
      "[Test 2] How do you claim undiscovered land?...\n",
      "Generated (337 chars): You can't just go and claim any empty land you find because most land already belongs to a country, ...\n",
      "API test complete!\n"
     ]
    }
   ],
   "source": [
    "# Test with sample questions\n",
    "print(f\"Testing API with {test_sample_size} sample questions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_questions = unique_questions.head(test_sample_size)\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    question = row['title']\n",
    "    print(f\"\\n[Test {idx + 1}] {question[:70]}...\")\n",
    "    answer = generate_deepseek_answer(question)\n",
    "    \n",
    "    if answer.startswith(\"ERROR\"):\n",
    "        print(f\"{answer}\")\n",
    "    else:\n",
    "        print(f\"Generated ({len(answer)} chars): {answer[:100]}...\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"API test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a3be8",
   "metadata": {},
   "source": [
    "## 6. Generate LLM Answer for Each Unique Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600f692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000 LLM answers with 6 concurrent workers...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 5000/5000 [39:44<00:00,  2.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Generated 5000 / 5000 LLM answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate one LLM answer per unique question using parallel processing\n",
    "llm_answers_map = {}  # Map from q_id to llm_answer\n",
    "\n",
    "questions_to_process = unique_questions_filtered.head(min(num_questions_to_generate, len(unique_questions_filtered)))\n",
    "\n",
    "def process_single_question(row):\n",
    "    \"\"\"\n",
    "    Process a single question and return (q_id, answer) tuple.\n",
    "    \"\"\"\n",
    "    q_id = row['q_id']\n",
    "    question = row['title']\n",
    "    \n",
    "    try:\n",
    "        llm_answer = generate_deepseek_answer(question)\n",
    "        \n",
    "        if not llm_answer.startswith(\"ERROR\"):\n",
    "            return (q_id, llm_answer, True)\n",
    "        else:\n",
    "            return (q_id, llm_answer[:80], False)\n",
    "    except Exception as e:\n",
    "        return (q_id, f\"ERROR: {str(e)[:80]}\", False)\n",
    "\n",
    "# Check if there are questions to process\n",
    "if len(questions_to_process) == 0:\n",
    "    print(\"No questions to process. All questions have been generated already.\")\n",
    "else:\n",
    "    print(f\"Generating {len(questions_to_process)} LLM answers with {max_workers} concurrent workers...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use ThreadPoolExecutor for concurrent API calls\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = {executor.submit(process_single_question, row): idx \n",
    "                   for idx, row in questions_to_process.iterrows()}\n",
    "        \n",
    "        # Process completed futures with progress bar\n",
    "        with tqdm(total=len(futures), desc=\"Generating answers\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                q_id, result, success = future.result()\n",
    "                \n",
    "                if success:\n",
    "                    llm_answers_map[q_id] = result\n",
    "                else:\n",
    "                    print(f\"\\nSkipped {q_id}: {result}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Rate limiting: brief delay after each completion\n",
    "                time.sleep(delay_between_api_calls / max_workers)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Generated {len(llm_answers_map)} / {len(questions_to_process)} LLM answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073fd4f",
   "metadata": {},
   "source": [
    "## 7. Create LLM Answers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c970629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new rows: 5000\n",
      "Unique questions: 5000\n",
      "Columns: ['q_id', 'title', 'text', 'source']\n"
     ]
    }
   ],
   "source": [
    "# Create LLM answers dataset\n",
    "llm_rows_list = []\n",
    "\n",
    "for q_id, llm_answer in llm_answers_map.items():\n",
    "    # Get the question info\n",
    "    question_info = unique_questions[unique_questions['q_id'] == q_id].iloc[0]\n",
    "    \n",
    "    llm_rows_list.append({\n",
    "        'q_id': q_id,\n",
    "        'title': question_info['title'],\n",
    "        'text': llm_answer,\n",
    "        'source': 'deepseek'\n",
    "    })\n",
    "\n",
    "df_llm = pd.DataFrame(llm_rows_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total new rows: {len(df_llm)}\")\n",
    "print(f\"Unique questions: {df_llm['q_id'].nunique()}\")\n",
    "print(f\"Columns: {list(df_llm.columns)}\")\n",
    "\n",
    "if len(df_llm) == 0:\n",
    "    print(\"\\nNo new answers generated in this run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe325a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample LLM answers:\n",
      "================================================================================\n",
      "\n",
      "[1] Question: How do you claim undiscovered land?...\n",
      "    Answer: You can't just go and claim any empty land you find because most of the world is already owned by co...\n",
      "    Source: deepseek\n",
      "\n",
      "[2] Question: Why do we fail to do realistic human CGI (like in SW Rouge One) yet we...\n",
      "    Answer: We can make amazing CGI animals and monsters because our brains aren't as picky about how they shoul...\n",
      "    Source: deepseek\n",
      "\n",
      "[3] Question: Why is it that we calm down when we take a deep breath, hold it for a ...\n",
      "    Answer: When you feel scared or upset, your body gets ready to run or fight, which makes your heart beat fas...\n",
      "    Source: deepseek\n"
     ]
    }
   ],
   "source": [
    "# Show sample rows\n",
    "print(\"\\nSample LLM answers:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in df_llm.head(3).iterrows():\n",
    "    print(f\"\\n[{idx + 1}] Question: {row['title'][:70]}...\")\n",
    "    print(f\"    Answer: {row['text'][:100]}...\")\n",
    "    print(f\"    Source: {row['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46114ad2",
   "metadata": {},
   "source": [
    "## 8. Save LLM Answers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c3ca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to: deepseek-output\\eli5_deepseek_answers_20260217_003444.csv\n",
      "Parquet saved to: deepseek-output\\eli5_deepseek_answers_20260217_003444.parquet\n",
      "Metadata saved to: deepseek-output\\metadata_20260217_003444.json\n",
      "Generated 5000 new LLM answers\n",
      "Output folder: deepseek-output\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_folder = \"deepseek-output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Only save if there are new answers generated\n",
    "if len(df_llm) > 0:\n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_filename = os.path.join(output_folder, f\"eli5_deepseek_answers_{timestamp}.csv\")\n",
    "    df_llm.to_csv(csv_filename, index=False)\n",
    "    print(f\"CSV saved to: {csv_filename}\")\n",
    "\n",
    "    # Save as Parquet\n",
    "    parquet_filename = os.path.join(output_folder, f\"eli5_deepseek_answers_{timestamp}.parquet\")\n",
    "    df_llm.to_parquet(parquet_filename, index=False)\n",
    "    print(f\"Parquet saved to: {parquet_filename}\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_rows': int(len(df_llm)),\n",
    "        'unique_questions': int(df_llm['q_id'].nunique()),\n",
    "        'deepseek_answers': int(len(df_llm)),\n",
    "        'llm_model': deepseek_model,\n",
    "        'max_workers': max_workers,\n",
    "        'columns': list(df_llm.columns)\n",
    "    }\n",
    "\n",
    "    metadata_filename = os.path.join(output_folder, f\"metadata_{timestamp}.json\")\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Metadata saved to: {metadata_filename}\")\n",
    "    print(f\"Generated {len(df_llm)} new LLM answers\")\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "else:\n",
    "    print(\"All questions have already been processed in previous runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecde28b",
   "metadata": {},
   "source": [
    "## 9. Merge all generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7995c756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - eli5_deepseek_answers_20260217_003444.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge all CSVs under deepseek folder\n",
    "output_folder = \"deepseek-output\"\n",
    "\n",
    "# Find all CSV files in the output folder\n",
    "csv_pattern = os.path.join(output_folder, \"eli5_deepseek_answers_*.csv\")\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"No CSV files found in {output_folder}\")\n",
    "else:\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "    \n",
    "    # Load and concatenate all CSV files\n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_file}: {str(e)}\")\n",
    "    \n",
    "    if dfs:\n",
    "        # Merge all dataframes\n",
    "        df_merged = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates based on q_id (keep first occurrence)\n",
    "        df_merged_unique = df_merged.drop_duplicates(subset=['q_id'], keep='first')\n",
    "       \n",
    "        # Save merged file\n",
    "        merged_filename = os.path.join(output_folder, \"eli5_deepseek_answers_merged.csv\")\n",
    "        df_merged_unique.to_csv(merged_filename, index=False)\n",
    "        \n",
    "        # Save as parquet\n",
    "        merged_parquet = os.path.join(output_folder, \"eli5_deepseek_answers_merged.parquet\")\n",
    "        df_merged_unique.to_parquet(merged_parquet, index=False)\n",
    "    else:\n",
    "        print(\"No data loaded to merge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
