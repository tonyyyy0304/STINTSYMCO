{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d3807e",
   "metadata": {},
   "source": [
    "# ELI5 Dataset Cleaning\n",
    "\n",
    "Dataset cleaning for entries with:\n",
    "\n",
    "- very short answers (20 words)\n",
    "- removal of URLs\n",
    "- removal of Reddit-specific artifacts (e.g. \"EDIT:\", \"OP\")\n",
    "- removal of multiple whitespaces\n",
    "- removal of emojis\n",
    "- removal of duplicates\n",
    "- removal of HTML artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281445f1",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c45c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1aa283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (261214, 7)\n",
      "Columns: ['q_id', 'title', 'category', 'subreddit', 'a_id', 'text', 'score']\n",
      "First row:\n",
      "     q_id                                              title category  \\\n",
      "0  5lchat  Why there was a 'leap second' added to the end...    Other   \n",
      "\n",
      "           subreddit     a_id  \\\n",
      "0  explainlikeimfive  dbuoyxl   \n",
      "\n",
      "                                                text  score  \n",
      "0  the rotation of the earth is not a constant. i...     44  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "input_file = 'output/eli5_combined.csv'\n",
    "output_file = 'output/eli5_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"First row:\\n{df.head(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c466b25",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb05fa3",
   "metadata": {},
   "source": [
    "## Define Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6569ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "def remove_reddit_artifacts(text):\n",
    "    # Remove common Reddit artifacts\n",
    "    text = re.sub(r'\\bEDIT\\b\\s*:?', '', text, flags=re.IGNORECASE) # EDIT:\n",
    "    text = re.sub(r'\\bOP\\b', '', text) # OP\n",
    "    text = re.sub(r'\\bETA\\b\\s*:?', '', text, flags=re.IGNORECASE) # ETA:\n",
    "    text = re.sub(r'\\bUPDATE\\b\\s*:?', '', text, flags=re.IGNORECASE) # UPDATE:\n",
    "    text = re.sub(r'\\bTL;DR\\b\\s*:?', '', text, flags=re.IGNORECASE) # TL;DR:\n",
    "    text = re.sub(r'\\bPS\\b\\s*:?', '', text, flags=re.IGNORECASE) # PS:\n",
    "    text = re.sub(r'^>+', '', text) # remove > at the start of lines (common in Reddit quotes)\n",
    "    text = re.sub(r'\\*([A-Z]+)\\*', r'\\1', text)  # remove asterisks from bold *A-Z* words\n",
    "    text = re.sub(r'\\( URL_[0-9]+ \\)', '', text)  # remove ( 'URL_[0-9]+' ) pattern for URLs in ELI5 data\n",
    "    text = re.sub(r'^\\s*\\*\\s+', '', text, flags=re.MULTILINE)  # remove bullet points ' * ' at the start of lines\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_html_artifacts(text):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def remove_no_question_mark_in_titles(df):\n",
    "    return df[df['title'].str.contains(r'\\?')]\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade475dc",
   "metadata": {},
   "source": [
    "## Apply Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5223bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column 'text' for cleaning\n",
      "\n",
      "Applying cleaning pipeline...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m df_cleaned = df_cleaned.drop_duplicates(subset=[text_column]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 8. Remove entries where the title does not contain a question mar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m df_cleaned[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_no_question_mark_in_titles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m before_title_filter = \u001b[38;5;28mlen\u001b[39m(df_cleaned)\n\u001b[32m     43\u001b[39m df_cleaned = df_cleaned[df_cleaned[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m] != \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mremove_no_question_mark_in_titles\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_no_question_mark_in_titles\u001b[39m(df):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.str.contains(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "# Assuming the answer column is named 'answer', 'text', or similar\n",
    "# First, let's identify the text column\n",
    "text_columns = [col for col in df_cleaned.columns if col.lower() in ['answer', 'text', 'content', 'body']]\n",
    "if not text_columns:\n",
    "    # If no standard name found, use the first text-like column\n",
    "    text_columns = [df_cleaned.columns[-1]]\n",
    "\n",
    "text_column = text_columns[0]\n",
    "print(f\"Using column '{text_column}' for cleaning\")\n",
    "\n",
    "# Apply cleaning steps\n",
    "print(\"\\nApplying cleaning pipeline...\")\n",
    "\n",
    "# 1. Remove URLs\n",
    "df_cleaned[text_column] = df_cleaned[text_column].astype(str).apply(remove_urls)\n",
    "\n",
    "# 2. Remove HTML artifacts\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_html_artifacts)\n",
    "\n",
    "# 3. Remove Reddit-specific artifacts\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_reddit_artifacts)\n",
    "\n",
    "# 4. Remove emojis\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_emojis)\n",
    "\n",
    "# 5. Remove multiple whitespaces\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_multiple_whitespaces)\n",
    "\n",
    "# 6. Filter out very short answers (< 20 words)\n",
    "initial_count = len(df_cleaned)\n",
    "df_cleaned = df_cleaned[df_cleaned[text_column].apply(count_words) >= 20].reset_index(drop=True)\n",
    "short_removed = initial_count - len(df_cleaned)\n",
    "\n",
    "# 7. Remove duplicates based on text column\n",
    "duplicates_removed = df_cleaned.duplicated(subset=[text_column]).sum()\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=[text_column]).reset_index(drop=True)\n",
    "\n",
    "# 8. Remove entries where the title does not contain a question mark\n",
    "before_title_filter = len(df_cleaned)\n",
    "df_cleaned = df_cleaned[df_cleaned['title'].astype(str).str.contains(r'\\?', na=False)].reset_index(drop=True)\n",
    "title_filtered = before_title_filter - len(df_cleaned)\n",
    "\n",
    "print(f\"Entries with < 20 words removed: {short_removed}\")\n",
    "print(f\"Duplicate entries removed: {duplicates_removed}\")\n",
    "print(f\"Entries without question marks removed: {title_filtered}\")\n",
    "print(f\"Final dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"\\nSample cleaned entries:\")\n",
    "print(df_cleaned[text_column].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9d229",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d416bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: output/eli5_cleaned.csv\n",
      "Original entries: 261214\n",
      "Final entries: 250208\n",
      "Entries removed: 11006\n",
      "Retention rate: 95.79%\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned dataset saved to: {output_file}\")\n",
    "\n",
    "print(f\"Original entries: {len(df)}\")\n",
    "print(f\"Final entries: {len(df_cleaned)}\")\n",
    "print(f\"Entries removed: {len(df) - len(df_cleaned)}\")\n",
    "print(f\"Retention rate: {(len(df_cleaned) / len(df) * 100):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
