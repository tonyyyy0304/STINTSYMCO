{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d3807e",
   "metadata": {},
   "source": [
    "# ELI5 Dataset Cleaning\n",
    "\n",
    "Dataset cleaning for entries with:\n",
    "\n",
    "- very short answers (20 words)\n",
    "- removal of URLs\n",
    "- removal of Reddit-specific artifacts (e.g. \"EDIT:\", \"OP\")\n",
    "- removal of multiple whitespaces\n",
    "- removal of emojis\n",
    "- removal of duplicates\n",
    "- removal of HTML artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281445f1",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c45c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a1aa283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (261214, 7)\n",
      "Columns: ['q_id', 'title', 'category', 'subreddit', 'a_id', 'text', 'score']\n",
      "First row:\n",
      "     q_id                                              title category  \\\n",
      "0  5lchat  Why there was a 'leap second' added to the end...    Other   \n",
      "\n",
      "           subreddit     a_id  \\\n",
      "0  explainlikeimfive  dbuoyxl   \n",
      "\n",
      "                                                text  score  \n",
      "0  the rotation of the earth is not a constant. i...     44  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "input_file = 'output/eli5_combined.csv'\n",
    "output_file = 'output/eli5_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"First row:\\n{df.head(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c466b25",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb05fa3",
   "metadata": {},
   "source": [
    "## Define Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6569ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "def remove_reddit_artifacts(text):\n",
    "    # Remove common Reddit artifacts\n",
    "    text = re.sub(r'\\bEDIT\\b\\s*:?', '', text, flags=re.IGNORECASE) # EDIT:\n",
    "    text = re.sub(r'\\bOP\\b', '', text) # OP\n",
    "    text = re.sub(r'\\bETA\\b\\s*:?', '', text, flags=re.IGNORECASE) # ETA:\n",
    "    text = re.sub(r'\\bUPDATE\\b\\s*:?', '', text, flags=re.IGNORECASE) # UPDATE:\n",
    "    text = re.sub(r'\\bTL;DR\\b\\s*:?', '', text, flags=re.IGNORECASE) # TL;DR:\n",
    "    text = re.sub(r'\\bPS\\b\\s*:?', '', text, flags=re.IGNORECASE) # PS:\n",
    "    text = re.sub(r'^>+', '', text) # remove > at the start of lines (common in Reddit quotes)\n",
    "    text = re.sub(r'\\*([A-Z]+)\\*', r'\\1', text)  # remove asterisks from bold *A-Z* words\n",
    "    text = re.sub(r'\\( URL_[0-9]+ \\)', '', text)  # remove ( 'URL_[0-9]+' ) pattern for URLs in ELI5 data\n",
    "    text = re.sub(r'^\\s*\\*\\s+', '', text, flags=re.MULTILINE)  # remove bullet points ' * ' at the start of lines\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_html_artifacts(text):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade475dc",
   "metadata": {},
   "source": [
    "## Apply Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5223bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column 'text' for cleaning\n",
      "\n",
      "Applying cleaning pipeline...\n",
      "Entries with < 20 words removed: 10954\n",
      "Duplicate entries removed: 52\n",
      "Final dataset shape: (250208, 7)\n",
      "\n",
      "Sample cleaned entries:\n",
      "0    the rotation of the earth is not a constant. i...\n",
      "1    The Earth's rotation is not regular. It varies...\n",
      "2    Because the Earth's rotation is slowing. If yo...\n",
      "3    Imagine you are out walking in the woods near ...\n",
      "4    By force. Historically, nations have defended ...\n",
      "Name: text, dtype: object\n",
      "Entries with < 20 words removed: 10954\n",
      "Duplicate entries removed: 52\n",
      "Final dataset shape: (250208, 7)\n",
      "\n",
      "Sample cleaned entries:\n",
      "0    the rotation of the earth is not a constant. i...\n",
      "1    The Earth's rotation is not regular. It varies...\n",
      "2    Because the Earth's rotation is slowing. If yo...\n",
      "3    Imagine you are out walking in the woods near ...\n",
      "4    By force. Historically, nations have defended ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "# Assuming the answer column is named 'answer', 'text', or similar\n",
    "# First, let's identify the text column\n",
    "text_columns = [col for col in df_cleaned.columns if col.lower() in ['answer', 'text', 'content', 'body']]\n",
    "if not text_columns:\n",
    "    # If no standard name found, use the first text-like column\n",
    "    text_columns = [df_cleaned.columns[-1]]\n",
    "\n",
    "text_column = text_columns[0]\n",
    "print(f\"Using column '{text_column}' for cleaning\")\n",
    "\n",
    "# Apply cleaning steps\n",
    "print(\"\\nApplying cleaning pipeline...\")\n",
    "\n",
    "# 1. Remove URLs\n",
    "df_cleaned[text_column] = df_cleaned[text_column].astype(str).apply(remove_urls)\n",
    "\n",
    "# 2. Remove HTML artifacts\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_html_artifacts)\n",
    "\n",
    "# 3. Remove Reddit-specific artifacts\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_reddit_artifacts)\n",
    "\n",
    "# 4. Remove emojis\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_emojis)\n",
    "\n",
    "# 5. Remove multiple whitespaces\n",
    "df_cleaned[text_column] = df_cleaned[text_column].apply(remove_multiple_whitespaces)\n",
    "\n",
    "# 6. Filter out very short answers (< 20 words)\n",
    "initial_count = len(df_cleaned)\n",
    "df_cleaned = df_cleaned[df_cleaned[text_column].apply(count_words) >= 20].reset_index(drop=True)\n",
    "short_removed = initial_count - len(df_cleaned)\n",
    "\n",
    "# 7. Remove duplicates based on text column\n",
    "duplicates_removed = df_cleaned.duplicated(subset=[text_column]).sum()\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=[text_column]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Entries with < 20 words removed: {short_removed}\")\n",
    "print(f\"Duplicate entries removed: {duplicates_removed}\")\n",
    "print(f\"Final dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"\\nSample cleaned entries:\")\n",
    "print(df_cleaned[text_column].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9d229",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d416bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: output/eli5_cleaned.csv\n",
      "Original entries: 261214\n",
      "Final entries: 250208\n",
      "Entries removed: 11006\n",
      "Retention rate: 95.79%\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned dataset saved to: {output_file}\")\n",
    "\n",
    "print(f\"Original entries: {len(df)}\")\n",
    "print(f\"Final entries: {len(df_cleaned)}\")\n",
    "print(f\"Entries removed: {len(df) - len(df_cleaned)}\")\n",
    "print(f\"Retention rate: {(len(df_cleaned) / len(df) * 100):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
